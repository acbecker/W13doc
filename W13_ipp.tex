\documentclass[12pt]{article}
\usepackage{epsfig}
\usepackage{natbib}
\usepackage{amssymb,amsmath}
\usepackage{titletoc}
\usepackage{color}

\usepackage[toc,page]{appendix}

% HOW TO SET UP AN 8.5 x 11:
% http://www.pages.drexel.edu/~pyo22/students/latexRelated/latexTutorial.html
\topmargin -1.5cm        % read Lamport p.163
\oddsidemargin -0.04cm   % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth 16.59cm
\textheight 21.94cm 
\parskip 7.2pt           % sets spacing between paragraphs
\parindent 0pt		     % sets leading space for paragraphs


\input{pythonlistings}
\definecolor{orange}{rgb}{1.00,0.65,0.00}
\def\arcsec{^{\prime\prime}}

\newcommand{\becker} { \textcolor{orange} {
\ensuremath{\blacksquare} {\bf AndyB:}  
\ensuremath{\blacksquare} } }

\newcommand{\simon} { \textcolor{red} {
\ensuremath{\bigstar} {\bf Simon:}  
\ensuremath{\bigstar} } }

\newcommand{\ajc} { \textcolor{blue} {
\ensuremath{\clubsuit} {\bf AndyC:}  
\ensuremath{\clubsuit} } }

\newcommand{\yusra} { \textcolor{cyan} {
\ensuremath{\diamondsuit} {\bf Yusra:}  
\ensuremath{\diamondsuit} } }

\newcommand{\russ} { \textcolor{green} {
\ensuremath{\natural} {\bf Russ:}  
\ensuremath{\natural} } }

%\titlecontents{subsection}[4.8em]{}{\contentslabel{2.4em}}{\hspace*{-2.8m}}{...}
%\titlecontents{subsubsection}[6.0em]{}{\contentslabel{2.4em}}{\hspace*{-2.8m}}{}

\author{Andrew Becker, Simon Krughoff, Andy Connolly}
\title{Roadmap for Winter2013 Production}
\date{\today}

\begin{document}

\maketitle

The goal of late Winter2013 production is the testing of image
subtraction algorithms and the exercise of the {\tt ip\_diffim}
package.  The package may be utilized at 3 distinct portions of the
production pipeline: in the subtraction of Snaps for the rejection of
cosmic rays; in the matching of input images for a Psf--matched
template; and in the subtraction of this template from individual
science Exposures.

The input data will be simulated LSST images (ImSim); stretch goals
include application of the algorithms to SDSS Stripe82 data (S82).
The processing steps for the ImSim data include: application of the
Isr pipeline; combining the two Snaps in a Visit into a single
Exposure; calibration, detection, and measurement on the Exposure;
stacking of multiple Exposures into a Psf--matched template image;
subtraction of this template from single--epoch Exposures; detection,
measurement, and characterization of DiaSources that remain in the
difference image; and ingestion of these DiaSources into a MySQL
database.  The natural QA hooks are in the assessment of the
Psf--matched template, and in the purity of the DiaSource sample.  We
will focus on the following metrics to assess the production:
\begin{itemize}
\item rate of false positives
\item rate of missed detections
\item performance (computational and I/O)
\end{itemize}
This document outlines the details of each stage of production,
including estimates of compute time, and addresses the anticipated
Configs for each Task.

\clearpage
\tableofcontents
\clearpage

%%%%%%%

\clearpage 
\section{ImSim Input Data} 

The sets of simulated images produced for Summer 2012 (which will be
the inputs for Winter 2013 late production) are described at \\{\tt
  http://dev.lsstcorp.org/trac/wiki/ImSim/Summer2012Plan}.  Processing
of SDSS Stripe82 data is a stretch goal for Winter 2013 production.

We wish to explore the capabilities of the image subtraction software
across the focal plane, and as a function of wavelength (both of the
sources and of the effective filter).  The former argues for
operations on (at a minimum) full quadrants of the focal plane, as
vignetting is known to attenuate the signal in the outer rafts.  The
latter argues for the analysis of observations in multiple passbands:
one band where chromatic effects are expected to be negligible
($i$--band) and one where the chromatic effects are expected to be
substantial ($g$--band).

The ImSim {\tt 5yr Run} simulated 2500 visits (full focal plane) of 5
adjacent fields over a 5-year period in the full suite of filters.
The simulations include both rotational and pointing dithering, and
the atmosphere includes no clouds or aerosols.  We will extract all of
the $i$--band data of the central field for a baseline study of the
image subtraction algorithms.  The $i$--band data will serve as a
reference set that maps out the underlying performance of the
algorithms in a regime where differential chromatic refraction (DCR)
within the bandpass is expected to be negligible.  The baseline spec
will be to generate a template image from the coaddition of the first
half of these data, with the appropriate cuts on conditions and
quality.  The second half of these data will be differenced with this
template, and the quality of the subtractions analyzed based on the
metrics above.  There are {\bf XX} $i$--band images of the central
field.

We will step through several sub--production runs to assess the
quality of the processing.  This includes sub--productions runs in the
following configurations:
\begin{itemize}
\item Images in $i$-band at low airmass to avoid the effects of DCR
\item Images in $i$-band over a short timescale to avoid proper motion effects
\end{itemize}

The second portion of production will move to the analysis of data in
the $g$--band, where DCR within the passband is expected at the scale
of the Psf.  This will inform future specifications for
color--dependent Psf--modeling and Psf--matching kernels.  There are
{\bf XX} $g$--band images of the central field.

Data will be prepared at various locations and stored at JHU with
images brought over as needed.  We can use standard grid copy tools
(gridftp, bbcp).  Before large runs start, the data will need to
staged at the compute center.  The estimated time of transfer is
(We'll need to benchmark this) demonstrated rate of {\bf XXX b/s} and
ImSim data volume of {\bf XXX GB} results in transfer time of {\bf
  XXX} days.  The registry generation time is ~10min per visit.  We
will need to solve the problem of how to point to the correct flat in
the registry given a rotator angle for the science image.  This has
not been done before.

We will be applying overscan correction, bias correction and flat
fielding to the ImSim data.  The dark current in the models is below
the level we need to worry about.  The flats will be normalized to a
global value.  The zeropoints should be consistent from chip to chip.
We {\it will} need rotation dependent flats due to vignetting.
Informed by John P. we should have flats in increments of 5 deg from 0
to 90.  Amps will be assembled in readout order such that the serial
transfer direction is along the x-axis of the image.

Master flats, darks, and biases will be produced by coadding 10
examples (each including 2 snaps).  We will not be simulating any time
dependent effects in the calibration products.

The spider introduces asymmetric vignetting.  This should be taken out
using flat fields produced with a variety of rotator angles.  Where
the effect is strongest, the angular dependence introduces scatter of
$\sim~5$\%.  Using flats generated at increments of 5 deg from 0 - 90
deg (18 master flats) should bring this below the nominal 1\% error
(although any remaining error will be systematic).

John will change the simulator to produce the chips in readout order
instead of camera coordinates so that the orientation of the eimage is
syncproniced with the assembled calexps.

\subsection{Issues}

The calibration production will be non-trivial.  We'll need to make
registries for $\sim~200$ visits and process them at $\sim~$minutes
per chip.  The new calibration product tasks will make this much
easier.  Simon is working on said tasks.  As of Dec 11, 2 days worth
of work before it can be submitted for review.

%%%%%%%

\clearpage 
\section{ProcessCcdTask} 

\subsection{Input/Outputs}

Outputs will be:
\begin{itemize}
\item Metadata for ingest
\item Src for ingest
\item Calexp for input to {\tt ip\_diffim}
\item Background to add to Calexp for {\tt ip\_diffim}
\item Psf for Psf matching
\end{itemize}


\subsection{IsrTask}
\subsection{SnapCombineTask}

The baseline LSST observing plans defines a field ``visit'' as the
acquisition of two back--to--back images (called Snaps) and their
combination into a single visit Exposure.  The reason for this is to
reject cosmic rays that are fall outside the detection thresholds of
single--image CR rejection routines.  By subtracting the two snaps
(snap2 - snap1), CRs may be detected with either positive or negative
polarity, indicating a CR in the second or first snap, respectively.
These pixels will be masked, and the two snaps coadded into the final
visit Exposure.

There are two options to creating the image difference on which CRs
will be detected.  The first is to take a straight difference of the
two images.  This should suffice under the assumption of a fully
realized Psf and stable atmosphere, and of sufficient precision in the
telescope tracking over the 32 second visit window.  The second option
is to use a Psf--matching kernel to register both the shapes and/or
positions of the stars if any of the assumptions above fail.  A
disadvantage of this method is that, due to the convolution inherent
in Psf--matching, the shapes of the CRs will be distorted, potentially
making them more difficult to detect.

The outstanding issue driving the decision to use Psf--matching or not
is the amount of image motion between Snaps in the data.  The
amplitude of these shifts in the Winter2013 test data, determined
through manual source detection and matching, is {\bf XX}$\arcsec$.
This amplitude of shifts suggests that image registration techniques
will be necessary for Winter2013 processing.  It remains an
outstanding issue (but beyond the scope of this planning document) if
these shifts reflect a realistic amount of image motion for an
LSST--like system.

The detection and masking of CRs will proceed as follows:

How do we make sure we only mask out undetected cosmic rays and not
differences in stars.  

The combined Exposure will have an exposure time equivalent to the sum
of the two input Snaps (i.e. this will {\it not} be an average image).
The FITS keywords that need to be modified in this process include:
{\bf XXX}.  The FITS keywords that will {\it not} be modified in this
process, but that reflect metadata that are different between the two
Snaps, include : {\bf WCS}.  These features will be copied from the
first snap of the visit pair.

\subsection{Calibrate/Detect/Measure subTasks}
The important parameters to consider are:
\begin{itemize} 
\item background grid size -- There are several of these in the config
  so we should understand what each one does
\item aperture size for aperture corrections -- the default is likely
  too small
\item deblending radius
\item galaxy shape parameters
\end{itemize}

\subsection{AstrometryTask} 

We already have a stars only reference catalog for the entire area
covered by the W13 ImSim area.  We will produce the galaxy catalog to
go along with this.  The reference catalog will contain the following
columns:

\begin{itemize}
\item id -- unique integer id of the object traceable back to the base catalogs
\item RA position of the object centroid in degrees
\item DEC position of the object centroid in degrees
\item {u,g,r,i,z,y} observed LSST magnitudes (in the mean sense for variable objects)
\item object class id (main sequence star, wd, bhb, etc.)
\item variability class (Agn, m-dwarf flare, RRly, etc. 0 if not variable)
\item galaxy position angle in degrees (do we measure these, or should we give these in both components)
\item galaxy semi-major axis 
\item galaxy semi-minor axis
\item bulge to total ratio
\item Agn to total ratio
\item disk to total ratio
\item Agn variability parameters (tau and SF\_{u,g,r,i,z,y})
\item A\_v reddening 
\end{itemize}

We will also need the reference catalog to be a function of time for
the time variable objects (brightness, position).  This has not been
done before.

\subsection{Issues}

\subsection{New Developments}
The reference catalog needs to have time--dependent entries.

\subsection{Tuning Parameters}

\subsection{Metrics and Testing}

\subsection{Tools}

%%%%%%%

\clearpage 
\section{MakeSkyMapTask} 

\subsection{Input/Outputs}
\subsection{Issues}
It would be helpful if we tested some fields that were at the edge of
the inner region of a tract.
\subsection{New Developments}
\subsection{Tuning Parameters}

\subsection{Metrics and Testing}
\subsection{Tools}

%%%%%%%

\clearpage 
\section{MakeCoaddTempExpTask} 

The input images will be Psf--matched to a designed template Psf
before coaddition.  This Psf will be a circular double Gaussian
function, with the smaller of the Gaussians having the FWHM of the
{\bf XX}$^{th}$ percentile of the input images' FWHM.  The outer
Gaussian will have a FWHM of {\bf twice?} the central Gaussian, and
contribute {\bf $10\%$} of the total flux.

\subsection{Input/Outputs}

\subsection{Issues}
We do not have color-dependent Psfs.  This will be minimized by
analysis of $i$--band data, and explored in analysis of $g$--band
data.

The {\tt meas\_mosaic}, which is a port of HSC's \"{u}ber cal, is
necessary to correct known deficiencies in LSST's {\tt meas\_astrom}
package.  This has proven necessary for creation of non--smeared
coadded images, reducing the RMS of the astrometric fit by a factor of
two.  This is leading to an additional systematic component equivalent
to {\bf XXX} in coadded images, and will vary spatially.

\subsection{New Developments}

\subsection{Tuning Parameters}
We will need to test the following Psf--matching configs:
\begin{itemize}
\item Psf size (set in processCcd)
\item Psf grid 
\item Psf model functions
\item kernel size
\end{itemize}

\subsection{Metrics and Testing}
\subsection{Tools}

%%%%%%%

\clearpage 
\section{AssembleCoaddTask} 

The coadded template image will be constructed using background
matching, as opposed to background subtraction.  The image subtraction
code will also be run using its own internal background--matching (of
template to science image) model, yielding a difference image with a
background level of 0.  Currently using a Chebyshev with order 3.

We need to make sure all necessary metadata ends up in the database.
This includes zeropoints, sky background, Psf FWHM.

\subsection{Input/Outputs}

\subsection{BackgroundMatchingTask} 

\subsection{Issues}

\subsection{New Developments}
Spreading of Mask bits.  How do we define the selection criteria, and
select, input and reference images from the database.

\subsection{Tuning Parameters}

\subsection{Metrics and Testing}

\subsection{Tools}

%%%%%%%

\clearpage 
\section{ProcessCoaddTask} 

To optimize the basis shapes matching the template image to a science
image, we require the Psf of the template image.  This level of
template processing also allows us to undertake QA to determine if the
realized shape of the Psf matches the specifications it was designed
to, if the astrometry of the template matches the SkyMap it was
designed to, and if there is any brightness dependence of the Psf by
looking at the measured brightness of reference catalog stars in the
template.  This essentially requires that we run calibrate, detection
and measurement on the template image, yielding at a minimum the {\tt
  Psf} and {\tt Source} products.

\subsection{Input/Outputs}

\subsection{Calibrate/Detect/Measure subTasks}

\subsection{Issues}

\subsection{New Developments}
Create zeropoint, Psf, and Wcs of coadd for QA, but do not overwrite
extant ones.

\subsection{Tuning Parameters}

\subsection{Metrics and Testing}

\subsection{Tools}
Means to compare fitted zeropoint, Psf, and Wcs of coadd with the
design specs.

%%%%%%%

\clearpage 
\section{ImageDifferenceTask} 
There are two classes of options for the selection of objects used to
model the Psf--matching Kernel.  The first involves use of a
starSelectorRegistry.  This may be a ``second moment'' star selector
to ensure that we only receive stars in the list, or a catalog--based
selector that queries a reference catalog for sources, and may be used
as a place--holder for a future specialized task that selects objects
that are non--variable and have negligible proper motion.  The second
option is to use native {\tt ip\_diffim} code that runs a very basic
detection algorithm and then sorts by integrated flux.  This code was
written long before starSelectorRegistries were implemented, and may
arguably be deprecated.  In both cases, objects with corresponding
Mask bits () set are ignored.

The number of sources that are detected in any given image determine
the complexity of the model that may be used to build a Psf--matching
kernel.  We need to decide if we are going to reduce the degree of the
spatial model based on the number of detected sources or not; this
means that the {\tt Config} will describe the heuristics used to
determine the spatial order of the models, and not the spatial order
of the models themselves.  This information needs to be persisted as
metadata if we choose to go this route.  We also need to establish the
minimum signal--to--noise of an object to be used in Kernel fitting.

By default, we will be using sum--of--Gaussian bases for Winter2013,
given the outstanding issue of how to build spatial models with delta
function bases.  The size of these Gaussians may be derived from the
FWHM of the template and science images using a heuristic; this
heuristic needs to be derived and trained.  The overall number of
bases becomes inflated by multiplying each basis with a set of
Hermite--polynomials.  The number of bases needed will be determined
using the Bayesian Information Criterion as a principled means to
decide if a basis is necessary or not.


\subsection{Input/Outputs}

\subsection{Calibrate/Detect/Measure subTasks}

\subsection{Issues}
We do not have color-dependent Psf-matching kernels.  

\subsection{New Developments}
We need a piece of code to query the database, find the overlapping
tracts, and coadd them into a template.  

Do we use RHL's option to pre--convolve.

\subsection{Tuning Parameters}
The default for Winter2013 is to use Chebyshev polynomials for spatial
interpolation.  A stretch goal will be to implement Gaussian processes
(kriging).
\subsection{Metrics and Testing}
\subsection{Tools}

%%%%%%%

\clearpage 
\section{Database Ingestion} 

\subsection{Input/Outputs}
\subsection{Calibrate/Detect/Measure subTasks}
\subsection{Issues}
\subsection{New Developments}
\subsection{Tuning Parameters}
\subsection{Metrics and Testing}
\subsection{Tools}

%%%%%%%

\clearpage 
\section{Estimated Processing Times} 

\begin{table*}[h]
\small
\begin{center}
\caption{\label{tab-pars} Estimated Processing Times}
\begin{tabular}{ccccc}
\hline \hline
Task                 & Time/Unit  & Units        & Number of Units & Total Time\\
\hline
Build Registry       & 10 minutes & Visit        & XXX             &           \\ 
ProcessCccdTask      &            & Ccd          & XXX             &           \\ % Sum of all 4 subtasks below
~~~IsrTask           & 1 minute   & Ccd          & XXX             &           \\
~~~SnapCombineTask   &            & Ccd          & XXX             &           \\
~~~Calexp CDM        &            & Ccd          & XXX             &           \\
~~~AstrometryTask    &            & Ccd          & XXX             &           \\
MakeSkyMapTask       & 10 seconds & Skymap       & 1               &           \\
MakeCoaddTempExpTask &            & Visit-Patch  & XXX             &           \\
AssembleCoaddTask    &            & Patch        & XXX             &           \\   
ProcessCoaddTask     &            & Patch        & XXX             &           \\
ImageDifferenceTask  &            & Ccd          & XXX             &           \\
Database Ingest      & 10$^{X}$/s & Rows         & XXX             &           \\
\hline
\hline
\end{tabular}
\end{center}
\end{table*}

%%%%%%%
%%%%%%%
%%%%%%%

\clearpage 
\begin{appendices}

\section{Flow Chart} \ajc

\section{Task Command lines and Configs}

\subsection{ProcessCcdTask}
\begin{python}
processCcd.py
\end{python}


\subsection{MakeSkyMapTask}
\begin{python}
makeSkyMap.py

--config
skyMap.name='dodeca'
skyMap.active.withTractsOnPoles=False
skyMap.active.projection='STG'
skyMap.active.patchInnerDimensions=4000,4000
skyMap.active.pixelScale=0.2
skyMap.active.patchBorder=250
skyMap.active.tractOverlap=3.5
\end{python}

\subsection{MakeCoaddTempExpTask} 
\begin{python}
makeCoaddTempExp.py
\end{python}

\subsection{AssembleCoaddTask} 
\begin{python}
assembleCoadd.py
\end{python}

\subsection{ProcessCoaddTask} 
\begin{python}
processCoadd.py
\end{python}

\subsection{ImageDifferenceTask} 
\begin{python}
imageDifference.py
\end{python}

\subsection{Database Ingestion} 
\begin{python}
$DATAREL_DIR/bin/ingest/ingestSources.py [lsstSim]
    ~becker/Winter2013/diffim_v0/tmpsdssdiff
    --host=lsst10.ncsa.illinois.edu 
    --database={yourDatabaseName} 
    --table={yourTableName}
    --dataset-type=goodSeeingDiff_src
    --id visit=873161311 raft=3,0 sensor=0,2
\end{python}
%$
\end{appendices}
%%%%%%%
%%%%%%%
%%%%%%%

\end{document}

















% Move the stuff below to above

%%%%%%%
\subsection{IsrTask (for ImSim)} \simon
Since there is no structure in the overscan, I do not believe we need
to do anything but the default for ISR.  That means overscan, bias,
flatfield, Cr interpolation, saturation and bad pixel corrections.
Right now the default for ImSim Isr is doDark=True.  This should be
fixed to reflect the fact that we no longer need dark correction with
the current model.

\subsubsection{What will be persisted}
In production we will only keep {\tt calexp}.  This means no {\tt
  postIsrCcd} will be persisted.  The Configs are automatically
persisted with the IsrTask.  Metadata to persist include: 
\begin{itemize}
\item nCR : the number of cosmic rays
\item median : the median of the calexp
\item image : 
\item gain : 
\item run time : 
\end{itemize}

\subsubsection{What is the processing time for ISR per Ccd}
The processing time is $\sim~1$ min per Ccd.

\subsubsection{What is the default Config for ISR}
\begin{python}
--config
doIsr=True
isr.doDark=False
\end{python}


\subsubsection{Which reference catalog will be used for the astrometry}

\subsubsection{Who will define it and create it from the sims}
This has been done based on the W13 surveys proposed on:
http://dev.lsstcorp.org/trac/wiki/ImSim/Summer2012Plan

\subsubsection{What depth is required for the reference catalog}
There is no reason to not go to 28$^{th}$ magnitude.  It's not that
big of an area.

\subsubsection{What format (and what processes) are required for the reference catalog}
This is very manual at the moment.  It would be nice to be able to
provide a CSV and schema to a routine that would produce both the
astrometry.net indexes and the match reach refObject.csv file.

\subsubsection{What metadata do we need to persist}
Metrics include: was the Wcs updated or did it default to the incoming
Wcs due to fitting failures; what is the RMS of the astrometric model
that was successfully fitted to the data.

\subsubsection{What is the default Config for AstrometryTask}
\begin{python}
--config
...
\end{python}




