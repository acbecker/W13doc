\documentclass[12pt]{article}
\usepackage{epsfig}
\usepackage{natbib}
\usepackage{amssymb,amsmath}
\usepackage{titletoc}
\usepackage{color}
\usepackage{graphicx}
\usepackage{rotating}

\usepackage[toc,page]{appendix}

% HOW TO SET UP AN 8.5 x 11:
% http://www.pages.drexel.edu/~pyo22/students/latexRelated/latexTutorial.html
\topmargin -1.5cm        % read Lamport p.163
\oddsidemargin -0.04cm   % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth 16.59cm
\textheight 21.94cm 
\parskip 7.2pt           % sets spacing between paragraphs
\parindent 0pt		     % sets leading space for paragraphs

\input{pythonlistings}
\definecolor{orange}{rgb}{1.00,0.65,0.00}
\def\arcsec{^{\prime\prime}}

\newcommand{\becker} { \textcolor{orange} {
\ensuremath{\blacksquare} {\bf AndyB:}  
\ensuremath{\blacksquare} } }

\newcommand{\simon} { \textcolor{red} {
\ensuremath{\bigstar} {\bf Simon:}  
\ensuremath{\bigstar} } }

\newcommand{\ajc} { \textcolor{blue} {
\ensuremath{\clubsuit} {\bf AndyC:}  
\ensuremath{\clubsuit} } }

\newcommand{\yusra} { \textcolor{cyan} {
\ensuremath{\diamondsuit} {\bf Yusra:}  
\ensuremath{\diamondsuit} } }

\newcommand{\russ} { \textcolor{green} {
\ensuremath{\natural} {\bf Russell:}  
\ensuremath{\natural} } }

\newcommand{\comment}[1]{{\color{cyan} [{comment: #1}]}}

%\titlecontents{subsection}[4.8em]{}{\contentslabel{2.4em}}{\hspace*{-2.8m}}{...}
%\titlecontents{subsubsection}[6.0em]{}{\contentslabel{2.4em}}{\hspace*{-2.8m}}{}

\author{Andrew Becker, Simon Krughoff, Andrew Connolly, Yusra AlSayyad, Russell Owen}
\title{Roadmap for Winter2013 Production}
\date{\today}

\begin{document}

\maketitle

The goal of late Winter2013 production is the testing of image
subtraction algorithms and the exercise of the {\tt ip\_diffim}
package.  The package will be utilized at 3 distinct portions of the
production pipeline: in the subtraction of Snaps for the rejection of
cosmic rays; in the matching of input images for a Psf--matched
template; and in the subtraction of this template from individual
science Exposures.

The input data will be subsets of the simulated LSST images (ImSim);
stretch goals include application of the algorithms to SDSS Stripe82
data (S82).  The processing steps for the ImSim data include:
application of the Isr pipeline; combining the two Snaps in a Visit
into a single Exposure; calibration, detection, and measurement on the
Exposure; stacking of multiple Exposures into a Psf--matched template
image; subtraction of this template from single--epoch Exposures;
detection, measurement, and characterization of DiaSources that remain
in the difference image; and ingestion of these DiaSources into a
MySQL database.  The natural QA hooks are in the assessment of the
Psf--matched template, and in the purity of the DiaSource sample.  We
will focus on the following metrics to assess the production:
\begin{itemize}
\item rate of false positives
\item rate of missed detections
\item performance (computational and I/O)
\end{itemize}
This document outlines the details of each stage of production, 
including estimates of compute time and required development, and addresses the anticipated
Configs for each Task.

\clearpage
\tableofcontents
\clearpage

%%%%%%%

\begin{sidewaysfigure}
\includegraphics[width=\textwidth]{Figures/Winter_2013.png}
\caption{Flow of primary information in image differencing. Brown
  boxes are input data, blue boxes represent the database, yellow
  represent ingestion into the database. Grey-brown boxes are derived
  data and yellow-green boxes are the individual pipeline tasks (applications
  that exist but not in the form of pipeline tasks are shown in pink).}
\label{flow}
\end{sidewaysfigure}

\clearpage 

\section{Simulated data} 

Three sets of simulated data are available for use in development of
difference imaging (5yr, Wide, and Deep). These are described in
detail at {\tt
  http://dev.lsstcorp.org/trac/wiki/ImSim/Summer2012Plan}.
Development of the LSST algorithms for Winter 2013 will focus on
subsets of the 5yr data, which comprise 2369 visits (full focal plane)
of 5 adjacent fields observed over a 5-year period in the full suite
of filters. All fields are dithered (spatially and rotationally), with the
spatial dithering defined with a minimum and maximum dither size of
0.2 and 1.75 degrees, respectively. Rotational dithering is based on
{\it rottelpos} (the angle of the camera with respect to the
telescope) as given by OpSim 3.61.

The depth maps derived from the positions of the 5yr data (including
dithers) for the $g$ (176 visits) and $i$ (466 visits) passbands are
shown in Figure~\ref{depth}. All observations are photometric with no
variation in transmission due to clouds. The airmass, sky brightness
and seeing associated with these pointings are given in
Figure~\ref{airmass}. Based on these distributions, the selections on
airmass and time described below all result in data sets of at least
20 visits (equivalent to one year of observations).

\begin{figure}
\centerline{
\includegraphics[width=0.4\textwidth]{Figures/depth_g.png}\hfil
\includegraphics[width=0.4\textwidth]{Figures/depth_i.png}
}
\caption{The pointing depth map (including dithers) for the 5yr data for the g (left panel)
 and i (right panel)  passbands.}
\label{depth}
\end{figure}


\begin{figure}
\centerline{
\includegraphics[width=0.3\textwidth]{Figures/airmass_mjd.png}\hfil
\includegraphics[width=0.3\textwidth]{Figures/sky_mjd.png}\hfil
\includegraphics[width=0.3\textwidth]{Figures/seeing.png}
}
\caption{The airmass, sky brightness, and seeing as a function of MJD for the
  5yr data in the g (blue points) and i (red points) passbands.}
\label{airmass}
\end{figure}

To evaluate the capabilities of the image subtraction software across
the focal plane, and as a function of wavelength (both of the sources
and of the effective filter), argues for operations on, at a minimum,
full quadrants of the focal plane (as vignetting attenuates the signal
in the outer raft) and the analysis of observations in multiple
passbands -- one band where chromatic effects are expected to be
negligible ($i$--band) and one where the chromatic effects are
expected to be substantial ($g$--band).

We accomplish this with a series of data sets drawn from the 5yr
simulations. We define a baseline data as the central CCD, in the $i$
band, at low airmass, and for a set of observations spread
over 30 days.  This will serve as the reference set that maps out the
performance of the algorithms relative to the optical design of the
LSST and variations in atmospheric seeing. Subsequent subsets of the
simulated data will address the dependence of the image differencing
on position on the focal plane, color, wavelength, time interval
between exposures, differential chromatic refraction, and high proper
motion sources. 
%1% in quadrature at 5 sigma

Figure~\ref{DCR} shows the differential chromatic refraction (DCR) as
a function of airmass (referenced to a G5V star) for a series of stars
ranging from O6 to M8. For main sequence stars in the current
simulated images, we expect the DCR shifts in the $i$ band to be
$<$0.1 pixels for airmass 1.3 and lower and $<$0.05 pixels for
airmasses less than 1.15. The corresponding DCR values for the $g$
band are $<$0.6 pixels and $<$0.4 pixels respectively. Baseline runs
will, therefore, be defined in the $i$--band at airmass $<$1.15.

\begin{figure}
\centerline{\includegraphics[width=0.8\textwidth]{Figures/DCR_R_stars.png}}
\caption{The differential chromatic diffraction relative to a G5V star
  as a function of airmass. The left panel shows the positional offset
  for the $g$ band and the right panel the $i$ band. In both panels
  the stars range in spectral type from O6 to M8.}
\label{DCR}
\end{figure}


\begin{table}
\begin{tabular}{llcrrr}
Sample & Device & Filter & Exposures & airmass & time interval \\
\hline  
Baseline CCD             & central         & $i$  & 20 & $<$1.15 &$<$30 days \\
Baseline Raft              & central raft  & $i$  & 20 & $<$1.15 &$<$30 days \\
Baseline Focal Plane   & focal plane  &  $i$ & 20 & $<$1.15 &$<$30
days \\
\hline  
Color CCD             & central         & $g$  & 20 & $<$1.15 &$<$30 days \\
Color Raft              & central raft  & $g$  & 20 & $<$1.15 &$<$30 days \\
Color Focal Plane   & focal plane  &  $g$ & 20 & $<$1.15 &$<$30 days \\
\hline 
Time CCD             & central         & $i$  & 20 & $<$1.15 &$>$3 years \\
Time Raft              & central raft  & $i$  & 20 & $<$1.15 &$>$3 years \\
Time Focal Plane   & focal plane  &  $i$ & 20 & $<$1.15 &$>$3 years \\
\hline 
DCR CCD             & central         & $g$  & 20 & $>$1.3 &$<$30 days \\
DCR Raft              & central raft  & $g$  & 20 & $>$1.3 &$<$30 days \\
DCR Focal Plane   & focal plane  & $g$  & 20 & $>$1.3 &$<$30 days
\end{tabular}
\end{table}


All specified data are currently stored at NCSA.  Special purpose runs
can and will be undertaken as needed using the Google resources and
will be staged at JHU and then transferred to NCSA.  

Master flats, darks, and biases will be produced by coadding 10
examples (each including 2 snaps).  We will not be simulating any time
dependent effects in the calibration products.

The spider introduces asymmetric vignetting.  This should be taken out
using flat fields produced with a variety of rotator angles.  Where
the effect is strongest, the angular dependence introduces scatter of
$\sim~5$\%.  Using flats generated at increments of 5 deg from 0 - 90
deg (18 master flats) should bring this below the nominal 1\% error.  

{\bf Reference Catalog}\\
We already have a stars--only reference catalog for the entire area
covered by the W13 ImSim area.  We will produce the galaxy catalog to
go along with this.  This reference catalog will contain the following
columns:

-- unique integer id of the object traceable back to the base catalogs

-- RA position of the object centroid in degrees

-- DEC position of the object centroid in degrees

-- {u,g,r,i,z,y} observed LSST magnitudes (in the mean sense for variable objects)

-- proper motion in RA marcsec/yr

-- proper motion in Dec marcsec/yr

-- object class id (main sequence star, wd, bhb, etc.)

-- variability class (Agn, M-dwarf flare, RRly, etc.  0 if not variable)

-- galaxy position angle in degrees 

-- galaxy semi-major axis 

-- galaxy semi-minor axis

-- bulge to total ratio

-- Agn to total ratio

-- disk to total ratio

-- Agn variability parameters (tau and SF\_{u,g,r,i,z,y})

-- A\_v reddening 


We will require the reference catalog to be a function of time for
the time variable objects (brightness, position).  This has not been
done before.

{\bf Stretch}\\
Processing of SDSS Stripe82 data is a stretch goal for Winter 2013
production.

%%%%%%%

\clearpage 
\section{CreateCalibrationProductsTask} 
This task takes individual calibration frames and combines them into
high signal to noise master frames.  We will run: masterbias, masterdark, masterflat.  
This task will not be camera specific and will be overridden by camera specific configs (and potentially
using the config to retarget subtasks) in the camera obs package.  The general flow of data will be to process the individual frames
with the appropriate instrument signature removal (ISR) steps for the product and camera.  Next a scale is computed for each of the input
frames.  Finally the frames are combined into the master frame.

\subsection{Input/Outputs}
Inputs are:

-- A set of raw calibration frames.

Outputs are:

-- A set of master calibration frames.

\subsection{CalibrationProcessTask subTask}
This is an ISR-like task that corrects the frames for the appropriate instrumental effects for that calibration
product.

\subsection{CalibrationScaleTask}
This task determines the scaling for a particular single epoch.  This task is responsible for
scaling all flats in a focal plane so that there is a single normalization for the full focal plane.

\subsection{CalibrationCombineTask}
The individual calibration frames are combined into a single high signal to noise master frame. 

\subsection{Evaluation and Development}
-- Most of this task has been implemented using a combination of ImSim specific code and code written by Paul P. for Subaru.

-- A design review will be scheduled to address any implementation issues.

-- The effectiveness of the angular dependent flat fields will be evaluated.


%%%%%%%

\clearpage 
\section{ProcessCcdTask\label{processccdsec}} 
The ProcessCcdTask does the work required to take input data from raw form all the way to a calibrated and
photometered science image.  The ProcessImageTask task is subclassed with camera specific configs and processing, but the overall 
work flow is first to do the instrument signature removal step which is handled by the camera specific subclass.  For ImSim
data, there is an additional step of combining the individual snap images before the rest of processing happens. Next calibration
is run.  This measures the Psf and aperuture correction, and also does the photometric and astrometric calibration.  
Detection comes next.  After detection, footprints are deblended.
Finally measurement is run on the deblended footprints.  Most of the above steps are optional, but care should be taken
when turning any one off that it does not affect down stream processing.

\subsection{Input/Outputs}
Inputs are:

-- Raw science frame

-- Astrometry.net index files for photometric and astrometric calibration

-- Master calibration frames


Outputs are:

-- Exposure metadata for ingest (bboxes, zeropoints, etc.)

-- Src for ingest

-- Calexp for input to {\tt ip\_diffim}

-- Background to add to Calexp for {\tt ip\_diffim}

-- Psf for Psf matching

\subsection{IsrTask subTask}
It was decided that the instrument signature removal was so dependent on the specific instrument setup that a generalized
ISR task was not generally useful.  The ISR is expected to be handled by a camera specific task in the camera obs package.  
For ImSim the process is overscan subtraction, bias correction, flat correction, assembly of amps into a chip--sized image, 
and interpolation of saturated and bad pixels.

\subsection{SnapCombineTask subTask}
The baseline LSST observing plans defines a field ``visit'' as the
acquisition of two back--to--back images (called Snaps) and their
combination into a single visit Exposure.  By subtracting the two snaps
(snap2 - snap1), CRs may be detected in the difference as objects with
either positive or negative polarity, indicating a CR in the second or
first snap, respectively.  These pixels will be masked and
interpolated, and the two snaps coadded into the final visit Exposure.


\subsection{CalibrateTask subTask} 
First off the calibration repairs any cosmic rays it finds.
The calibrate task does an initial optional background subtraction and detection phase (which also does a background background subtraction).  The detected sources are used to calculate
the Psf, the aperture correction, the photometric zeropoint, and are fed to the astrometry.net framework for astrometric
calibration.

\subsection{DetectTask subTask}
The detect task does an optional second background estimation taking into account the original detection mask set by the calibrate task.
It then runs final detection.

\subsection{DeblendTask subTask}
The deblend task searches for peaks within the footprints returned by the detect task and splits the children out into deblended sources.

\subsection{MeasureTask subTask}
The measure task makes the statistical measurements on each footprint (or peak if deblended).

\subsection{Evaluation and Development}

-- A primary issue is which steps in processCcd can be turned off.  In our experience with Stripe82, we found that the products produced by
essentially all steps were necessary in later processes.  The one possible exception was the deblendTask.  Aperture corrections (CalibrateTask) 
are needed by forced photometry, zeropoints (CalibrateTask)
are needed by pipeQA and makeCoaddTask, and detection planes (DetectTask) are needed by the background matching tasks.

{\bf ImSim IsrTask}\\
-- Saturation detection is done before bad pixel masking because we have the pixel masks in CCD coordinates.
This meant that some bad pixels were also marked as SAT.  We fix this by changing all SAT+BAD pixels back to just BAD.

-- The flats had per chip normalizations meaning that the zeropoint was not fixed for the focal plane.  This should be
fixed by the new calibration products task

-- Amps will be assembled in readout order such that the serial transfer direction is along the x-axis of the image.

-- Implement a mechanism for the butler to retrieve the appropriate calibration products given a camera rotator angle.

{\bf SnapCombineTask}\\

-- The outstanding issue driving the decision to use Psf--matching vs. a straight difference
is the amount of image motion between Snaps in the W13 ImSim data.
The amplitude of these shifts in the Winter2013 test data, determined
through manual source detection and matching, is up to 0.12$\arcsec$
(0.6 pixels).  This amplitude of shift requires that Psf--matching techniques will be necessary for Winter2013 processing.  

-- Establish if the default spatial order (0) for this model is sufficient.

-- Establish if the delta function kernel basis set may be used in this Task.
 The delta--function kernel set is more sensitive
to noise than sum--of--Gaussian basis sets, but better able to model
fractional pixel shifts without significant shape distortions.  The
zero--order spatial model will help to regularize the overfitting
problem.

-- Establish a sufficient set of basis shapes for the sum--of--Gaussian basis.
Given the prior expectation that the matching Kernel will look like a delta function, 
this basis set will be smaller than the default ImagePsfMatchTask set (31 terms).

-- Decide between the two bases in the final SnapCombineTask Conig.

-- Establish sufficient dimensions for the Psf--matching kernel, since again
the power is expected to be localized.

-- Develop tools to distinguish cosmic rays from the most significant
contaminant, the cores of bright stars.  

-- Develop tools to query the known cosmic ray list (locations, shapes and amplitudes), 
and associate them with DiaSources.

-- Devlop tools to query the known star list, and associate them with DiaSources.

-- Develop cuts on measurement features to distinguish cosmics from
stellar cores in the Snap DiaSources.

-- Establish the final metadata for the combined Exposure.  E.g. the
combined Exposure will have an exposure time equivalent to the sum of
the two input Snaps (i.e. this will {\it not} be an average image).  Establish
the metadata to be summed (e.g. exposure time), averaged (e.g. TAI of exposure), 
and assumed to be the same (e.g. Wcs).

{\bf CalibrateTask}\\
-- Determine an optimial background grid size; there are several of these in the config
  and we must understand what each one does.

-- Determine the aperture sizes for aperture corrections; the default is too small.

{\bf DeblendTask}\\
-- Establish the deblending radius.

{\bf MeasurementTask}\\
-- Determine which galaxy shape parameters are to be measured.

\subsection{Metrics and Testing Tools}

-- Evaluate the cross--match the list of DiaSources  with the cosmic ray list, and
with the known star list.  The measured features of these two samples
will be compared against each other to find the most distinguishing
characteristics.  We will calculate the efficiency and purity of this
sample as metrics to optimize.

%%%%%%%

\clearpage 
\section{MakeSkyMapTask} 

Creates a "sky map": a sky pixelization for coadds consisting of a collection of overlapping "tracts".
Each tract is, essentially, a very large exposure with its own WCS.
Tracts are subdivided into rectangular overlapping subregions called "patches".

The sky map we plan to use is DodecaSkyMap. It has 12 tracts arranged as the faces of a dodecahedron.
An alternative if we decide we want more smaller tracts is HealpixSkyMap which arranges
tracts as pixels in a HEALPix arrangement.

\subsection{Input/Outputs}

As for all tasks, specify an input and (optionally) output data repository.
makeSkyMap creates a sky map in the output repository.
A repository may have two sky maps: one for "deep" coadds and one for "goodSeeing" coadds.

\subsection{Evaluation and Development}

DodecaSkyMap has large tracts which have significant distortion at the edges.
We don't believe this will be an issue for image differencing because the template is warped
to match the science exposure before subtracting. If it does prove to be a problem
we can switch to using HealpixSkyMap with smaller tracts.

SkyMap unpersistence has been a bit troublesome due to pickling {\it pex\_config} fields.
When {\it pex\_config} changes sometimes this breaks SkyMap unpersistence. We must fix this eventually.
I don't think it's a high priority, but we could try to fix it in time for late Winter2012 production.

None. If we do decide to switch to HealpixSkyMap then we must make healpy an LSST package.
Healpy is self-contained other than needing cfitsio.

The overlap between tracts may be larger than necessary, but the main effect of that
will be to slightly increase storage requirements of coadds and slightly increase
the time required to generate them. We will not see either effect in late Winter2013
because we are only working with a small region on the sky.

\subsection{Metrics and Testing Tools}

No testing is required.  Skymap examples/plotSkyMap prints out useful
information about a sky map and displays a 3d graph of it.

%%%%%%%

\clearpage 
\section{MakeCoaddTempExpTask} 

Suitable science images for coaddition will be selected based on Psf FWHM, 
sky level, and photomeric zeropoint.  These images will then be Psf--matched to a model Psf and warped
to the WCS of the desired coadd patch to produce a coadd temp exposure.
The model Psf will be a circular double Gaussian
function, with variable {\tt sigma1} reflecting the desired FWHM of the coadd.  This will
be chosen from the distribution of FWHM of the input data, third quartile for depth and second
quartile for good seeing.

All CCDs from a visit that overlap a particular patch are combined into one coadd temp exposure,
so we will create one coadd temp exposure per patch per visit.
These coadd temp exposures are later assembled into a coadd by AssembleCoaddTask.

\subsection{Input/Outputs}

Inputs include science exposures ("calexp"), a database providing information about
those calexp, selection criteria, the desired coadd ("deep" or "goodSeeing"),
the skymap for that coadd and model Psf parameters.

The output is a set of coadd temp exposures ("deepCoadd\_tempExp" or "goodSeeingCoadd\_tempExp").

\subsection{SelectImagesTask subTask} 

This subTask selects science exposures that are suitable for the coadd
based on criteria such as Psf FWHM and sky background. It operates by
querying the Science\_Ccd\_Exposure table.

\subsection{ScaleZeroPointTask} 

This subTask  photometrically calibrates each {\tt coaddTempExp} to a common zero point. 
%
In the coadd process, it is important that the background matcher not try to fit discontinuous offsets in the background levels. For this reason, the multiplicative scale factor applied to the coaddTempExp should vary smoothly across the patch. For stripe-82 early production we assumed that the zero-point in the runs varied smoothly in the R.A direction only.  This will be expanded to a full two--dimensional model in W13.  The zeropoints are already calculated in {\tt processCcd} and stored in the Science\_Ccd\_Exposure table. {\tt ScaleZeroPointTask} pins the zeropoint down in the center of each ccd and interpolates a multiplicative scale factor across the patch. 

\subsection{WarpAndPsfMatchTask subTask} 

This subTask first undertakes the Psf--matching of the input {\tt
  calexp} Psf to that desired for the Coadd.  It then astrometrically
warps this Psf--matched image to the astrometric footprint of the
Coadd.  This differs from ImagePsfMatchTask, described in detail in
Section~\ref{sec-imagedifftask}, in one major detail.  In this case,
the {\tt calexp's} {\it model} Psf is read through the butler, and
compared to the idealized Psf that we wish to realize for the Coadd.
Each Psf is realized in a grid of $x,y$ positions to account for
spatial variation of the {\tt calexp} Psf (the Coadd Psf model is
designed to not vary).  The two Psfs will be entered into a
KernelCandidate as Images, which it itself entered into a
SpatialCellSet, and sent along to the fitting code where the
functionality is again the same as ImagePsfMatchTask.  Because the
concept of ``variance'' is ill--defined when comparing
model--to--model, all sigma clipping is turned off and the variance is
assumed to have a constant weight.


\subsection{Evaluation and Development}

-- Improve the current Science\_Ccd\_Exposure table and SelectImagesTask.
The only measure of image quality that Science\_Ccd\_Exposure contains is Psf FWHM.
We would like additional information such as Psf ellipticity, airmass and sky brightness.

-- Develop an equation that computes a single measure of quality
based on these various bits of information.

-- Develop a tool that measures how well Psf matching to a model Psf
works. This requires running Psf modelling on a coaddTempExp.


{\bf ScaleZeroPointTask}

--  Fit a 2-D spline to the zeropoints calculated for each ccd in {\tt processCcd}.   {\tt afw} already contains 2-D spline capabilities, however the only interface at the moment is the {\tt background} class. The API to the background class only accepts an image. We need to extend this  to be able to accept a list of grid points.   

-- As a stretch goal, interpolate the scale factor across the patch using the zeropoint of each star. This is an appropriate application for a Gaussian Process/Kriging function. 

{\bf WarpAndPsfMatchTask}

-- There is trade--off in this process that we will explore in detail.  
To illustrate this, we define the dimensions of
the {\tt calexp} Psf as $P \times P$, the FWHM of the Coadd Psf as
$C$, and the dimensions of the Psf--matching kernel as $K \times K$.
When fitting for the matching Kernel, each Psf image gets convolved
with each Kernel basis function, trimming $K//2$ pixels off of each
side due to EDGE effects.  Thus the final number of pixels we have to
constrain the Kernel coefficients are $(P-K) \times (P-K)$.  This
means that as the Kernel gets larger, the number of pixels we have to
constrain the solution gets smaller, and the solution may become
unstable.  Providing tension in the other direction is that if the
kernel dimensions are too small compared to $C$, the kernel stamp is
not large enough to capture all the power needed to match to that
large of a FWHM.  This ends up yielding ``boxy'' stars in the
Psf--matched images.  Ultimately, we are limited by the dimensions of
the {\tt calexp} Psf model $P$, which in turn sets the maximum size of
the kernel such that $(P-K) \times (P-K)$ provides sufficient ability
to constrain all of the Kernel basis coefficients, which in turn sets
the size of the Psf $C$ we may match to without sufficiently impacting
the shapes of the stars, which in turn sets the number of images that we 
may include in the coadd without deconvolving.  This trade--off, including the definitions
of ``sufficient'' above, will be explored in thorough detail in W13
pre--production.

%-- If the model--to--model Psf--matching kernel is too big, we have
%too few pixels to constrain the solution.

%-- If the model--to--model Psf--matching kernel is too small, it
%limits the FWHM of the desired Psf of our coadd (otherwise the stars
%become boxy on the scale of the kernel).

%-- If there is a limitation on the FWHM of the coadd Psf, we are
%limited in the number of images we may use with (FWHM$_i$ $<$
%FWHM$_{Coadd}$), impacting our final depth.

%-- If we wish to increase our depth and include images in the coadd
%with (FWHM$_i$ $>$ FWHM$_{Coadd}$), we will have to deconvolve.

-- Understand ``how much'' deconvolution the code can handle,
both in terms of the fraction of images that were deconvolved, and the
degree to which each of the images is deconvolved, using the measured 
depth, variance, and Psf in the Coadd.

-- Establish the quartiles of the FWHM distribution to use for setting the Coadd Psf, 
and for including in the Coadd.

-- Establish the impact of {\it not} having color--dependent Psfs (or Psf--matching kernels) on the coadd.

-- Science images are presently Psf--matched to a model that is constant in pixel space.
However, we want to match to a model Psf that is constant on the sky.  The
distortion near the edge of the LSST's field of view may be large enough
that this distinction will prove significant. Jim Bosch is working on code
that will eliminate this issue by allowing us to warp model Psfs.
However, it is likely that we will produce some preliminary data before this new code is ready.

-- Understand the impact of the {\tt meas\_mosaic} package, which is a port of HSC's \"{u}ber
cal, and is necessary to correct known deficiencies in LSST's {\tt
  meas\_astrom} package.  This has proven necessary for creation of
non--smeared coadded images, reducing the RMS of the astrometric fit
by a factor of two.  

-- Implement minimal spreading of Mask bits in afwMath::convolve.

%We will need to test the following Psf--matching configs:
%
%-- Psf size (set in processCcd)
%
%-- Psf grid 
%
%-- Psf model functions
%
%-- kernel size
%
%We will need to tune our image selection criteria (possibly including our image quality metric)
%to select suitable images.

\subsection{Metrics and Testing Tools}

-- Develop tools to measure the Psf of the coaddTempExposure.

-- Establish the stability of the model--to--model Psf--matching
kernel as a function of Psf dimension and Kernel dimension by tracking
the condition number of the solution.

-- Use the Coadd Psf shape, noise properties, and photometric depth 
to determine the impacts of deconvolution.

-- Examine the shapes of extreme--color stars in the Coadd to understand the effects of DCR.

-- Evaluate the image quality metric developed for SelectImagesTask.

%%%%%%%

\clearpage 
\section{AssembleCoaddTask} 

The coadded template image will be constructed using background
matching, as opposed to background subtraction. The assembleCoadd task assembles a coadd from a set of coadd temporary exposures.   Each {\tt coaddTempExp} is the size of a patch defined by the {\tt skyMap} and contains pixels from only a single visit.  The {\tt coaddTempExp}s have already been scaled to a common zeropoint and psf-matched.  However, they still have their backgrounds and therefore require background-matching to a reference visit. The reference visit is selected based primarily on fuller coverage of the patch and low background levels and background variance. 

The assembleCoadd task assumes that all {\tt coaddTempExp}s cannot fit in memory at the same time. The following steps outline the processing in more detail:

-- Read in {\tt coaddTempExp}s one at a time and fill a list of weights (inverse mean variance) for each image 

-- Pass to the background matching task a list of dataRefs of the {\tt coaddTempExp}s, and the reference exposure dataRef (if supplied).  Receive back a list of structs containing {\tt Background} objects and statistics describing the quality of the match.  Prune this list of exposures, removing the coaddTempExps for which the background-matching failed.  Current tests include too many nans, and the ratio MSE/Var did not make the quality threshold.

-- Grid the patch into a smaller sub-region {\tt subBBox}es. For each {\tt subBBox}, read in each image and keep in memory only the region that overlaps the {\tt subBBox}. Add the corresponding region from the Background match to bring the background level up to that of the reference image.  Coadd by calculating the mean with optional outlier rejection.  Repeat for {\tt next subBBox}.  Persist coadd. 

%The image subtraction
%code will also be run using its own internal background--matching (of
%template to science image) model, yielding a difference image with a
%background level of 0.  Currently using a Chebyshev with order 3.

%We need to make sure all necessary metadata ends up in the database.
%This includes zeropoints, sky background, Psf FWHM.

\subsection{Input/Outputs}

Inputs: A repository containing coadd temporary exposures and a skyMap

Outputs: Coadds

\subsection{BackgroundMatchingTask subTask} 
The matchBackground task matches the background of each coadd temp exposure to the level of the reference exposure.   This is done by fitting a spatial model to the difference of each input exposure and the reference exposure -- with detections and bad pixels masked out.  This will equalize the background levels from visit to visit, while preserving diffuse astrophysical structures.  The task assumes that all the input images are the same size, have been warped and have been flux-scaled. 

Subtract the exposure from the reference exposure, and then pass generate an afw.math.Background object of this difference image. It assumes (but does not require or check) that the mask plane already has detections set.  The 'background' of the difference image is a collection of grid points that will be smoothed by spline interpolation (by the Background class)  or by polynomial interpolation by the Approximate class when an image is requested from it later. 

Inputs: 

-- A list of dataRefs  pointing to {\tt coaddTempExp}s and an optional dataRef to the reference exposure. 

Outputs:  

-- backgroundModel: an afw.math.Approximate or an afw.math.Background (Or {\tt None} if fit failed) 

-- fitRMS: rms of the fit, sqrt(mean(residuals**2))

-- matchedMSE: the MSE of the reference and matched images, mean((refImage - matchedSciImage)**2); 
   this value should be comparable to the mean variance of the difference image.

-- diffImVar: the mean variance of the difference image.

\subsection{Evaluation and Development}

-- Development: Move flux-scaling procedure to {\tt makeCoaddTempExp.py}.  Currently, the assembleCoadd performs the flux-scaling step. This design was a temporary solution to an early-production problem.  It would be much more efficient and clean for the flux scaling to be performed in makeCoaddTempExp.

-- Development:  Move the background-matching quality cut to the {\tt matchBackgrounds} subtask (hours work).

-- Development:  Check that API of background matching task is appropriate for {\tt ImageDifferenceTask} to match the science image to the template before subtracting. 

-- Evaluation: Determine the optimal background matching style (polynomial, spline) and parameters (order, bin size) for the 4000$\times$4000 patches. Differences between stripe82 data and ImSim data (e.g. the chip gaps and offsets between CCDs) prevent us from simply reusing the parameters from early production.

-- Development (out of scope): Write a task that will select a reference run based on the database that optimizes continuity over a large region of the sky. By large region, we mean $>>$ patch. 

\subsection{Metrics and Testing Tools}

-- To test the quality of fit in the background matching step, {\tt matchBackgrounds} contains a debugging method that plots the residuals of the fit.  In addition, the statistics returned for each fit (MSE and variance of the difference image) allow assessment of the interpolation styles/parameters.

%%%%%%%

\clearpage 
\section{ProcessCoaddTask} 
We will be running calibration, detection and measurement on coadds to 
generate the data products necessary for QA and also to measure the Psf
for matching science images to the template.  With the outputs of the process coadd task we will answer questions including:

-- Does the measured shape of the Psf match the input model Psf to which the template was matched?

-- Does astrometry of the template match the SkyMap?

-- How does the Psf change with position, source flux, etc.?

\subsection{Inputs/Outputs}

Inputs: \\
-- Coadd exposure

Outputs:\\
-- Coadd calexp

-- Coadd exposure metadata

-- Background model

-- Psf model

-- Source catalog

\subsection{Calibrate/Detect/Measure subTasks}
These are identical to those mentioned in \S \ref{processccdsec}.

\subsection{Evaluation and Development}

-- Establish the optimal grid spacing for background estimation.

-- Determine the deblending radius.

-- Establish to use image variance or per pixel variance for the detection threshold.

-- Determine the accuracty of measurement without a color dependent Psf.

-- Ingest the appropriate patch information in the coadd database table for ImageDifferencingTask.

-- There is no notion of persisting models for QA purposes alone.  For example we will want to persist
the WCS we measure, but only for comparing to the skymap WCS.  In other words, we do not want to replace the 
WCS but we do want to measure it.  

\subsection{Metrics and Testing Tools}

-- Use PipeQA to compare fitted zeropoint, Psf, and Wcs of Coadd with the design specs.

%%%%%%%

\clearpage 
\section{ImageDifferenceTask \label{sec-imagedifftask}} 


%Science evaluation in different conditions: as a function of seeing, S/N, airmass, rotation angle, filter, position on the focal plane, ?position in the tract? (should not matter), SED of star or galaxy, sensitivity to false positives due to moving objects.

The ImageDifferenceTask subtracts a template image that is
warped and Psf--matched to an input science image, from that science image.  The
image difference (or difference image) will contain only those objects
that have varied in position or brightness w.r.t. the template.
Detection and measurement will be run on the difference image to
characterize the residuals, both at positive and negative polarity,
and persisted as DiaSources.  This is implemented via a series of
subTasks: ImagePsfMatchTask, SourceDetectionTask, SourceDeblendTask,
SourceMeasurementTask.

In detail, the bounding box of a given input {\tt calexp} will be used
to query the Coadd database via the ImageDifferenceTask.getTemplate
method.  The multiple SkyMap patches that overlap this bounding box
will be assembled into a single Exposure.  All
subsequent pixel operations -- astrometric registration and
Psf--matching -- will happen on this template, leaving the {\tt
  calexp} pixels untouched.  

The Task next selects the objects to be used to Psf match the {\tt
  calexp} and template.  The code is currently configured to use a
starSelectorRegistry ``second moment'' star selector; the type of selector is configurable.  
Selection is run using the template image,
since this is likely to have higher signal--to--noise, and no
astrophysical transients.  Objects with Mask
bits in either image (set in the Config, currently ``EDGE'' and
``SAT'') are removed from the returned Source catalog.

The code next runs the ImagePsfMatchTask.  This accepts the two
Exposures, the FWHM of their two Psfs, and the Source catalog as
inputs.  ImagePsfMatchTask solves for $K(x,y)$ in the
equation $S(x,y) = (K \otimes T)(x,y)$, where $S$ is the science
calexp, $T$ is the template, and $K$ the Psf--matching kernel, by
performing a linear decomposition of $K$ such that $K(u,v) = \sum_i
a_i K_i(u,v)$.  The basis functions $K_i$ are derived using a
heuristic based on the FWHM of the input images; thus the challenge is to solve for $a_i$.
Additionally, the Psf--matching kernel is known to vary spatially
across the image, thus the general problem is to solve for $a_i(x,y)$.
This is accomplished by solving locally for $a_i$ at the positions
$x,y$ of each object in the Source catalog, and creating a spatial
model of these coefficients using a Chebyshev polynomial of order $N$.
We thus have {\it two} solutions that we will use to ascertain the
quality of the Psf--match: using the original coefficients $a_i$; and
using the interpolated coefficients $a{'}_i$ that are derived from the
spatial model at $x,y$.  The metrics we will derive from these
solutions are the mean and root--mean--square values of: the diffim
residuals normalized by the square root of the variance.  We will use
the residuals of a control sample of objects that are selected by the
object selector, but not used in the actual kernel fit, to determine
the overall quality of the Psf--matching solution.

This process results in a subtracted Exposure (the difference image).
The detection planes of this image, inherited from the {\tt calexp},
will be wiped clean in preparation for detection and measurement.  The
Psf of the science image will be used by SourceDetectionTask,
configured to find sources of both positive and negative polarity.
SourceMeasurementTask will be performed on the difference image source
catalog, using the aperture correction from the science image.  We
will run a DipoleChecker to search for and flag
dipoles.  The results will be persisted as DiaSources.  We require a
new subTask that associates these DiaSources with the reference
catalog {\it and} the {\tt calexp} source catalog.  DiaSources will be written to the database,
along with reference catalog and Source matches.  The Source match
table is a new database requirement.

\subsection{Input/Outputs}

Inputs are:

-- A {\tt calexp} containing the science data, along with its {\tt
  Psf} and {\tt Wcs}.

-- A set of Coadds that cover the bounding box of the {\tt calexp}.

Outputs are:

-- Difference Exposure (data product; detection planes are set)

-- DiaSources (data product)

-- Psf--matched Image (metadata in pipeBase.Struct)

-- Psf--matching Kernel (metadata in pipeBase.Struct)

-- Background match (metadata in pipeBase.Struct)

-- KernelCellSet (metadata in pipeBase.Struct)

\subsection{ImagePsfMatchTask subTask}
As described above, this subTask controls the actual Psf-matching of
the {\tt calexp} and the template.  We will be using a
sum--of--Gaussian basis for the matching Kernel, and a heuristic to
choose both the Gaussian shapes within this basis and the degree of
Hermite polynomials used to modify the Gaussians.

\subsection{SourceDetectionTask subTask}
This subTask uses the Psf of the input {\tt calexp} to run detection
on the difference Exposure.  The detection Config will be set to
search for objects of both positive and negative polarity.  Estimation
and re--estimation of the background will both be disabled.

\subsection{SourceDeblendTask subTask}
This subTask by default is turned off as it has not been tested on detections
with both positive and negative polarity.

\subsection{SourceMeasurementTask subTask}
As described above, this subTask performs measurement of the source
catalog returned by SourceDetectionTask.  It requires the aperture
correction of the {\tt calexp}.

\subsection{Evaluation and Development}

This may in some cases result in
deconvolution of the template image.  There is a general sense that
the template may be deconvolved more reliably than the single--depth
images, due to its higher signal--to--noise.  Quantifying this is a
research task for W13.

The FWHMs are used in a heuristic to determine the shape of
the bases used to model the Psf--matching kernel.  This heuristic will
be tuned on sample runs for W13.  The overall number of bases becomes
inflated by multiplying each basis with a set of Hermite--polynomials.
The number of bases ultimately used will be determined in
pre--production by using a principled means of deciding how much
information each basis contributes to the solution.  

-- We do not have color--dependent Psf--matching kernels,
e.g. $K(x,y,g-r)$, which are needed in the presence of differential
chromatic refraction.  This will impact the quality of the
Psf--matching happening in ImagePsfMatchTask, in particular for
objects of extreme color in the bluer passbands, and at high airmass.
We will first evaluate the fitting of $K(x,y)$ in the absence of
DCR by operating on $i$--band data, and estimate the degradation in
the quality of the solution when going to $g$--band.  Mitigation
strategies include creation of templates at different airmasses, but
this also requires sets both East and West of the meridian.

-- Objects with high proper motion will leave streaked images in the
Coadd, and result in a complex dipole structure in an image
difference.  We will initially mitigate these effects by analyzing
images from within a single observing season.

-- What is the minimum useful signal--to--noise of an object to be
used in Kernel fitting.

-- How do we prevent over-- or under--fitting of the spatial model?
The number of sources that are detected in any given image determine
the complexity of the model that may be supported.  We need to decide
if we are going to reduce the degree of the spatial model based on the
number of detected sources or not; this means that the {\tt Config}
will describe the heuristics used to determine the spatial order of
the models, and not the spatial order of the models themselves.  This
information needs to be persisted as metadata if we choose to go this
route.

-- We need to establish how much the template may be deconvolved in
the case that the science data have a narrower FWHM.  Alternately, we
need to understand the impacts of convolving the science image on the
depth we may detect to in the difference image.

-- We will turn off the majority of the measurements for the star
selection, in the interest of speed.

-- Tune the heuristic that selects the shapes of the Gaussians based
on the FWHM of the {\tt calexp} and the template.

-- Evaluate the number of Kernel bases needed as a function of Psf
FWHM and variation.

-- Formalize metrics and quality ranges to assess the goodness of fit
of the spatial kernel using the control sample.

-- Implement these metrics in pipeQA.

-- Define the database query to specify the overlapping tracts that
will be used for template generation (stub exists in
ImageDifferenceTask.getTemplate)

-- Add a dipole flag for the DiaSources.

-- Create a Task for source association between the DiaSources and the
reference catalog sources, and with the {\tt calexp} Sources.  The
latter also requires a new table in the database.

-- Debug ingest of the DiaSources into MySQL.

-- Evaluate RHL's option to pre--convolve.

-- Stretch goal: develop Gaussian Processes based interpolation and
evaluate against polynomial interpolation

-- Tune the heuristic that chooses the shapes of the Kernel basis
based on the input FWHMs.

-- Tune the size of the Kernel basis using a method that quantifies
degree of overfitting (e.g. BIC).

-- The degree of the spatial kernel model.  Optionally allow it to
float based on the number of KernelCandidates.

-- The degree of the spatial background--matching model ({\it not} the
same as BackgroundMatchingTask).

-- Use a per--pixel variance, or a single variance for the entire
image.

\subsection{Metrics and Testing Tools}

-- In pre--production, testing of the heuristic that defines the shape
of the Gaussians.

-- In pre--production, testing of the number of basis functions needed
to provide a quality fit.  

-- The fundamental quality metric we have been using is the
distribution of residuals in the difference image, normalized by the
square--root of the variance.  We will examine the value of this
metric for the per--object fitted coefficients, as well as the
spatially interpolated coefficients, to determine how much the spatial
model degrades the fit.  We will also define a control sample of
objects to ascertain the final quality of the image subtraction.

-- Efficiency and purity of object detection and measurement will be
done by comparing the DiaSources to the (time--dependent) reference
catalog.

-- False positives (most likely to be associated with bright stars)
will be investigated by looking at their properties in the database
(colors and proper motions).

Important metrics that will be exercised pre--production include: the
use of the BIC to determine the complexity of the kernel basis; an
examination of objects with large residuals in subtractions that we
expect to be nearly exact (what are their colors and proper motions).

In production our core metrics will include: what are the residuals of
a control sample of stars after application of the Psf--matching
kernel.

-- Several debugging hooks have been implemented in {\tt ip\_diffim} for
the visualization of the inputs and outputs to ImageDifferenceTask.

-- Persist appropriate metrics to enable the above testing in pipeQA.

%%%%%%%

\clearpage 
\section{Database Ingestion} 
This step is essentially a set of by hand operations.  
There will be a sourceIngestionTask for W13.  This is only one aspect of the
database ingestion, however.  The steps for the different phases of ingestion will be:\\
{\bf Registry Creation:}

-- Run genInputRegistry.py (possible in parallel)

-- Merge input registries by hand

{\bf SFM Source Ingestion:}

-- Run ingestProcessed.py

-- Run sourceAssociation.py

-- Run ingestSourceAssociation.py

-- Run referenceMatch.py

{\bf Coadd Source Ingestion:}

-- Run ingestCoadd.py

-- Run referenceMatchCoadd.py

{\bf Forced Source Ingestion:}

-- Run ingestForcedSource.py

\subsection{Evaluation and Development}
-- genInputRegistry.py takes ~10 min per visit to run.  With large amounts of data, this is intractable.  Evaluate the feasability of unifying parallel input registry generation.

-- Each of the above needs to be taskified and made so that it can be parallelized

-- In particular, streamlining the parallel process is a must if data volumes are $>$ 10E8 rows.

-- After parallel ingest, the tables need to be merged and indexed which can take a long time.  We need to identify the optimal recipe for parallel ingest and evaluate the options for automating this process.

-- Logging is done purely by printing to stdout, so it is possible to miss failures.  We need to evaluate other loggin options.

\subsection{Metrics and Testing Tools}

-- Tool to monitor ingestion rate (and predict finish time?).

-- Tool for determining missing data/failures

-- Tool for streamlining the merge process after parallel ingest.

%%%%%%%

\clearpage 
\section{Summary} 

The ``tall poles'' that will be evaluated in the development portion of production are:

-- Distinguish cosmic rays from the cores of stars in SnapCombine.

-- Establish dimensions of the matching kernel in WarpAndPsfMatchTask.

-- Determine complexity of the Kernel basis and spatial model in ImagePsfMatchTask.

-- Identify Sources of false positives in the difference images.


%%%%%%%

\clearpage 
\section{Estimated Processing Times} 

\begin{table*}[h]
\small
\begin{center}
\caption{\label{tab-pars} Estimated Processing Times}
\begin{tabular}{lccccl}
\hline \hline
Task                          & Time/Unit     & Units        & No. Units & Total Time & Notes \\
\hline
Build Registry                & 10 min        & Visit        & XXX             &        &  \\ 
ProcessCcdTask                & 110 sec + ?   & Ccd          & XXX             &        &  \\ % Sum of all 4 subtasks below
~~~~~~IsrTask                 & 30 sec        & $\cdots$     & $\cdots$        &        &  \\
~~~~~~SnapCombineTask         & ?             & $\cdots$     & $\cdots$        &        &  \\
~~~~~~Calexp CDM              & 80 sec        & $\cdots$     & $\cdots$        &        &  \\
~~~~~~AstrometryTask          & 1 sec         & $\cdots$     & $\cdots$        &        &  \\
MakeSkyMapTask                & 10 sec        & Skymap       & 1               &        &  \\
MakeCoaddTempExpTask          & 180 min       & Visit-Patch  & XXX             &        & 79 CCDs \\
AssembleCoaddTask             & 150 sec       & Patch        & XXX             &        & 9 coaddTEmpExp \\   
ProcessCoaddTask              & ?             & Patch        & XXX             &        &  \\
ImageDifferenceTask           & ?             & Ccd          & XXX             &        &  \\
~~~~~~ImagePsfMatchTask       &               & $\cdots$     & $\cdots$        &        &  \\
~~~~~~SourceDetectionTask     &               & $\cdots$     & $\cdots$        &        &  \\
~~~~~~SourceMeasurementTask   &               & $\cdots$     & $\cdots$        &        &  \\
~~~~~~SourceDeblendTask       &               & $\cdots$     & $\cdots$        &        &  \\
\hline
Database Ingest               & 10$^{-X}$ sec & Rows         & XXX             &        &  \\
~~~~~~Calexp                  &               & Rows         & XXX             &        &  \\
~~~~~~Coadd                   &               & Rows         & XXX             &        &  \\
~~~~~~DiaSource               &               & Rows         & XXX             &        &  \\
\hline
\hline
\end{tabular}
\end{center}
\end{table*}

%%%%%%%
%%%%%%%
%%%%%%%

\clearpage 
\begin{appendices}
\section{Task Command lines and Configs}

\subsection{ProcessCcdTask}
\begin{python}
processCcd.py

--config
snapCombine.diffim.kernel='DF'
snapCombine.doSimpleAverage=False
snapCombine.doPsfMatch=True
\end{python}


\subsection{MakeSkyMapTask}
\begin{python}
makeSkyMap.py

--config
coaddName='deep' # or 'goodSeeing'
skyMap.name='dodeca'
skyMap.active.withTractsOnPoles=False
skyMap.active.projection='STG'
skyMap.active.patchInnerDimensions=[4000, 4000]
skyMap.active.pixelScale=0.2
skyMap.active.patchBorder=250
skyMap.active.tractOverlap=3.5
\end{python}

\subsection{MakeCoaddTempExpTask} 
\begin{python}
makeCoaddTempExp.py

--config
coaddKernelSizeFactor=3.0
warpAndPsfMatch.psfMatch.kernel='AL'
\end{python}

\subsection{AssembleCoaddTask} 
\begin{python}
assembleCoadd.py
\end{python}

\subsection{ProcessCoaddTask} 
\begin{python}
processCoadd.py
\end{python}

\subsection{ImageDifferenceTask} 
\begin{python}
imageDifference.py
\end{python}

\subsection{Database Ingestion} 
\begin{python}
$DATAREL_DIR/bin/ingest/ingestSources.py [lsstSim]
    ~becker/Winter2013/diffim_v0/tmpsdssdiff
    --host=lsst10.ncsa.illinois.edu 
    --database={yourDatabaseName} 
    --table={yourTableName}
    --dataset-type=goodSeeingDiff_src
    --id visit=873161311 raft=3,0 sensor=0,2
\end{python}
%$

%%%%%%%

\clearpage 

\end{appendices}
%%%%%%%
%%%%%%%
%%%%%%%

\end{document}

